# ゼロから作るDeepLearning2 自然言語処理偏

- 公式ソースコード
    - https://github.com/oreilly-japan/deep-learning-from-scratch-2/tree/master

1. ニューラルネットワークの復習
2. 自然言語処理の単語の分散表現
3. word2vec
4. word2vecの高速化
5. RNN
6. ゲート付きRNN
7. RNNによる文章生成
8. Attentionの仕組み

## 1. ニューラルネットワークの復習
## 2. 自然言語処理の単語の分散表現

- 単語の意味を表現する手法
    - `シソーラス`：付録
        - 単語の意味ネットワークを辞書として使う
        - WordNet
            - 1985年にプリンストン大学で開発がスタート
        - 問題点
            - 人力で意味ネットワークを作るので作業コストが高い
            - 時代の変化などに対応するのが困難
            - 単語の細かなニュアンスが把握できない
        - 実装は付録にある(まだやってない)
    - `カウントベース`：本章
        - コーパスという，NLPの研究やアプリケーションの開発のために収集された大量のテキストデータから共起行列を作成し，ベクトル表現された各単語のベクトルのコサイン類似度で単語同士の類似度を比較する
        - 共起行列は単純なカウントベースで不完全であるので，相互情報量を用いる
        - 相互情報量を計算した行列がサイズが大きくスパースなため，特異値分解SVDなどを用いて密な行列に変換して使用する
    - `推論(word2vec)`：3章
        - 次章で詳しく

## 3. word2vec

- この章ではシンプルなword2vecを実装し．より高速で実用的なものは次章で実装
- カウントベースの欠点
    - 計算効率が悪い
- 推論ベースの利点
    - ミニバッチで並列計算できる
    - ミニバッチにするので，NNの重みの更新や，分散表現の追加などが効率的に行える
    - カウントベースは単語の類似度をエンコードするだけだが，word2vecは単語間の複雑な関係性も学習できている(king - man + woman = queen のような類推問題が解けるなどという有名な話がある)
    - しかし，精度という点においては，どちらが優れているかは明確ではないとされている
- 推論ベースの手法
    - 周囲の単語(コンテキスト)が与えられたときに穴埋めをするようなイメージ
    - その推論にニューラルネットワークを使う
        - 本章ではシンプルな2層のNNであるCBOWモデルを実装する

## 4. word2vecの高速化

- より高速なword2vecを実装し，PTBデータを対象に学習を行う
    - Embeddingの追加
    - 損失関数 Negative Sampling の追加
- 単語ベクトルの評価方法
    - 類似性を評価する際は，人間が事前につけたスコアと，コサイン類似度の相関を見る
    - 推測の精度では，king:queen=actor:{actress}みたいな感じで()を予測できるかみたいな評価をするらしい
- そもそもword2vecは何に活用できるのか
    - 大量のメールからクラスタに分類するとか

## 5. RNN

## 6. ゲート付きRNN

## 7. RNNによる文章生成

## 8. Attentionの仕組み

